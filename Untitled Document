

For comparing with our proposed method, I think of using the ijcai-19 to learn the embedding vectors, and compare the AUCs score. However, what function should I use for link prediction score? The ijcai-19 use the same objective function (objective of skip-gram model with negative sampling) to learn the embedding, but they use a logistic regression classifier for link prediction in their model. :S



??? what do you think about the scaling of CENALP model for larger and denser graphs. Currently the paper only have experiments with 1000 and 2000 nodes. (4000 nodes in case of Douban)
    



Dear Xingbo Du,

Thank you so much for your reply.

Actually, the similar results of using Link Prediction and not using Link Prediction makes me question about the correctness of my implementation a lot!!! :S

For cross-graph embedding, I just sample random walks accroding to the rules in the paper and use gensim library's Word2vec skip-gram model with negative sampling to learn the embedding.

For network alignment, when there is no seed nodes |S|=|S'| = 0, I only calculate node pair similarity for all nodes using sim_emb. And when the number of seed nodes |S|=|S'| > 0, I calculate node pair similarity only for the seed nodes using sim_emb and sim_jc. 
I believe that line 1 of Alg.1 refers to "Compute F = {sim(u,v)} for u \in S and v \in S'" instead of all the nodes when we have some seed nodes |S|=|S'| > 0. Please correct me if I misunderstood this part.
By the way, when matching nodes in each iteration, after we already match all node pairs with sim(u,v) > 0 and the number of matched pairs is still smaller than T, do you keep matching node pair (u,v) with sim(u,v)=0?

For link prediction, I do predict links only between the seed nodes. At the beginning, when |S|=|S'| is small, only few new links are predicted. However, when |S|=|S'| is around 500, 750, and 1000, LP starts to predict around 2000, 5000, and 8000 new links (which is still quite few compared to |S|*|S|). In the end, the number of edges still increase from less than 5000 to over 21,000. :((
Is it a normal behavior? 

I dont know if it is a proper thing to ask for, but if it is possible, could you please also share the DBLP dataset (2151 nodes) with me? 
I am afraid that maybe there are some mistakes in my implementation that accidentally make it work on the Facebook-Twitter dataset.


I am sorry for all the bothering.

Once again, I am very grateful for your detailed explanations and great patience with me.

Best regards,
Luu Huu Phuc


